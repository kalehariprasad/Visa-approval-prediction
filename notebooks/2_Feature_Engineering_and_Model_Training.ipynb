{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4437a980",
      "metadata": {
        "id": "4437a980"
      },
      "source": [
        "# Data Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74b4d7e0",
      "metadata": {
        "id": "74b4d7e0"
      },
      "source": [
        "#### Import Packages and CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5530cc59",
      "metadata": {
        "id": "5530cc59",
        "outputId": "5fa0529c-5adf-480a-b56f-116e14b8514e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import dagshub\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.pandas.set_option(\"display.max_columns\", None)\n",
        "# Create Dataframe\n",
        "df = pd.read_csv(\"EasyVisa.csv\")\n",
        "# Print shape of dataset\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cad163d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "mlflow.set_tracking_uri(\"https://dagshub.com/kalehariprasad/Visa-approval-prediction.mlflow\")\n",
        "dagshub.init(repo_owner='kalehariprasad', repo_name='Visa-approval-prediction', mlflow=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c27265bd",
      "metadata": {
        "id": "c27265bd"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbad805a",
      "metadata": {
        "id": "dbad805a"
      },
      "source": [
        "### Handling Missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a0c1c0d",
      "metadata": {
        "id": "0a0c1c0d"
      },
      "source": [
        "* Handling Missing values \n",
        "* Handling Duplicates\n",
        "* Check data type\n",
        "* Understand the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40b4a428",
      "metadata": {
        "id": "40b4a428"
      },
      "source": [
        "#### Check Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "2b94aa8f",
      "metadata": {
        "id": "2b94aa8f"
      },
      "outputs": [],
      "source": [
        "##these are the features with nan value\n",
        "features_with_na=[features for features in df.columns if df[features].isnull().sum()>=1]\n",
        "for feature in features_with_na:\n",
        "    print(feature,np.round(df[feature].isnull().mean()*100,5), '% missing values')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f08d8e60",
      "metadata": {},
      "outputs": [],
      "source": [
        "features_with_na"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31bb11b3",
      "metadata": {
        "id": "31bb11b3"
      },
      "source": [
        "* **There are no null values in the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76eafe12",
      "metadata": {
        "id": "76eafe12"
      },
      "source": [
        "### 3.2 Other Data Cleaning steps"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "471fd48f",
      "metadata": {
        "id": "471fd48f"
      },
      "source": [
        "**Handling Duplicates**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8fa17e0",
      "metadata": {
        "id": "d8fa17e0",
        "outputId": "7f1d6a37-65e2-4b4f-f201-69b9c3a80150"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f574d4c",
      "metadata": {
        "id": "3f574d4c"
      },
      "source": [
        "* **No Duplicates in the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cf6d275",
      "metadata": {
        "id": "5cf6d275"
      },
      "source": [
        "**Remove case_id from the dataset as it cannot used in Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "828c0a89",
      "metadata": {
        "id": "828c0a89"
      },
      "outputs": [],
      "source": [
        "df.drop('case_id', inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d48a184",
      "metadata": {
        "id": "6d48a184"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8453d379",
      "metadata": {
        "id": "8453d379"
      },
      "source": [
        "## Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40ec4ef6",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "8eac04de",
      "metadata": {
        "id": "8eac04de"
      },
      "outputs": [],
      "source": [
        "# importing date class from datetime module\n",
        "from datetime import date\n",
        "  \n",
        "# creating the date object of today's date\n",
        "todays_date = date.today()\n",
        "current_year= todays_date.year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9cc4d34",
      "metadata": {},
      "outputs": [],
      "source": [
        "current_year"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79bd9cbf",
      "metadata": {
        "id": "79bd9cbf"
      },
      "source": [
        "**Subtract current year with year of estab to get company's age**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "3c193e26",
      "metadata": {
        "id": "3c193e26"
      },
      "outputs": [],
      "source": [
        "df['company_age'] = current_year-df['yr_of_estab']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd661e95",
      "metadata": {
        "id": "fd661e95",
        "outputId": "5221c83c-9303-43b7-e12a-8e6f46d14f15"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "2cc2b5c1",
      "metadata": {
        "id": "2cc2b5c1"
      },
      "outputs": [],
      "source": [
        "df.drop('yr_of_estab', inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66542de1",
      "metadata": {
        "id": "66542de1"
      },
      "source": [
        "### Type of Features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85df52e5",
      "metadata": {
        "id": "85df52e5"
      },
      "source": [
        "**Numeric Features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d48aeaa4",
      "metadata": {
        "id": "d48aeaa4",
        "outputId": "3472d509-3613-408a-b2e0-7e04e3ba694a"
      },
      "outputs": [],
      "source": [
        "num_features = [feature for feature in df.columns if df[feature].dtype != 'O']\n",
        "print('Num of Numerical Features :', len(num_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1107060",
      "metadata": {
        "id": "e1107060"
      },
      "source": [
        "**Categorical Features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff997805",
      "metadata": {
        "id": "ff997805",
        "outputId": "3dd63e00-7af0-48de-d7bb-5d1f51c557de"
      },
      "outputs": [],
      "source": [
        "cat_features = [feature for feature in df.columns if df[feature].dtype == 'O']\n",
        "print('Num of Categorical Features :', len(cat_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bc032f9",
      "metadata": {
        "id": "9bc032f9"
      },
      "source": [
        "**Discrete features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "812ee6e0",
      "metadata": {
        "id": "812ee6e0",
        "outputId": "0181bd9a-5f2b-4292-8a57-d9c683d2e128"
      },
      "outputs": [],
      "source": [
        "discrete_features=[feature for feature in num_features if len(df[feature].unique())<=25]\n",
        "print('Num of Discrete Features :',len(discrete_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e6740bf",
      "metadata": {
        "id": "3e6740bf"
      },
      "source": [
        "**Continues Features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e501c72b",
      "metadata": {
        "id": "e501c72b",
        "outputId": "c10010b4-eb51-406e-afc4-464894031222"
      },
      "outputs": [],
      "source": [
        "continuous_features=[feature for feature in num_features if feature not in discrete_features]\n",
        "print('Num of Continuous Features :',len(continuous_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a2cf140",
      "metadata": {
        "id": "5a2cf140"
      },
      "source": [
        "### Split X and Y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9455c01a",
      "metadata": {
        "id": "9455c01a"
      },
      "source": [
        "* **Split Dataframe to X and y**\n",
        "* **Here we set a variable X i.e, independent columns, and a variable y i.e, dependent column as the â€œCase_Statusâ€ column.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "4434aa31",
      "metadata": {
        "id": "4434aa31"
      },
      "outputs": [],
      "source": [
        "X = df.drop('case_status', axis=1)\n",
        "y = df['case_status']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67a428d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "y.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2260600",
      "metadata": {
        "id": "f2260600"
      },
      "source": [
        "**Manual encoding target column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "7a9d7c95",
      "metadata": {
        "id": "7a9d7c95"
      },
      "outputs": [],
      "source": [
        "# If the target column has Denied it is encoded as 1 others as 0\n",
        "y= np.where(y=='Denied', 1,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "398fad76",
      "metadata": {},
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca2d817d",
      "metadata": {
        "id": "ca2d817d"
      },
      "source": [
        "## Feature Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a921c3a0",
      "metadata": {
        "id": "a921c3a0",
        "outputId": "84d46867-1130-41fc-9dca-e5087fccb83d",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# distribution of data before scaling\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, col in enumerate(['no_of_employees','prevailing_wage','company_age']):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    sns.histplot(x=X[col], color='indianred')\n",
        "    plt.xlabel(col)\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "993fca30",
      "metadata": {
        "id": "993fca30"
      },
      "source": [
        "* No of employees and Copmany age column is skewed\n",
        "* Apply a power transform featurewise to make data more Gaussian-like.\n",
        "\n",
        "Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired.\n",
        "\n",
        "Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a890a255",
      "metadata": {
        "id": "a890a255"
      },
      "source": [
        "**Checking Skewness**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca6ac67f",
      "metadata": {
        "id": "ca6ac67f"
      },
      "source": [
        "**What is Skewness ?**\n",
        "\n",
        "* Skewness refers to a distortion or asymmetry that deviates from the symmetrical bell curve, or normal distribution, in a set of data. If the curve is shifted to the left or to the right, it is said to be skewed. Skewness can be quantified as a representation of the extent to which a given distribution varies from a normal distribution. A normal distribution has a skew of zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4038a207",
      "metadata": {
        "id": "4038a207",
        "outputId": "51497045-4d93-46f8-ea18-47d742680845"
      },
      "outputs": [],
      "source": [
        "# Check Skewness\n",
        "X[continuous_features].skew(axis=0, skipna=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1786c0ad",
      "metadata": {
        "id": "1786c0ad"
      },
      "source": [
        "- Positiviely Skewed : company_age, no_of_employees.\n",
        "- We can handle outliers and then check the skewness."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96c39509",
      "metadata": {
        "id": "96c39509"
      },
      "source": [
        "## Apply Power Transformer to Check if it can reduces the outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "56ad9567",
      "metadata": {
        "id": "56ad9567"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PowerTransformer\n",
        "pt = PowerTransformer(method='yeo-johnson')\n",
        "transform_features = ['company_age', 'no_of_employees']\n",
        "X_copy = pt.fit_transform(X[transform_features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "255b1c5e",
      "metadata": {
        "id": "255b1c5e"
      },
      "outputs": [],
      "source": [
        "X_copy = pd.DataFrame(X_copy, columns=transform_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b49bad36",
      "metadata": {
        "id": "b49bad36",
        "outputId": "d732f372-b0f2-4bed-9f4d-394d46891abb"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "for i, col in enumerate(transform_features):\n",
        "    plt.subplot(1, 2, i+1)\n",
        "    sns.histplot(x=X_copy[col], color='indianred')\n",
        "    plt.xlabel(col)\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb26baf6",
      "metadata": {
        "id": "bb26baf6"
      },
      "source": [
        "**Checking Skewness**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c9a232a",
      "metadata": {
        "id": "4c9a232a",
        "outputId": "1ef33497-82c9-42b1-d17d-c62b5c5049ad"
      },
      "outputs": [],
      "source": [
        "X_copy.skew(axis=0, skipna=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69c717fc",
      "metadata": {
        "id": "69c717fc"
      },
      "source": [
        "- Here Yeo-Johnson is used and it supports both positive or negative data for transformation.\n",
        "- So Power Transformer with yeo-johnson can be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7740ff27",
      "metadata": {
        "id": "7740ff27",
        "outputId": "76d06e1f-cc70-41fa-91e2-6b99a2683eb4"
      },
      "outputs": [],
      "source": [
        "for feature in cat_features:\n",
        "    print(feature,':', df[feature].nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51a3853d",
      "metadata": {
        "id": "51a3853d"
      },
      "source": [
        "## Feature Encoding and Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29dbaf2d",
      "metadata": {
        "id": "29dbaf2d"
      },
      "source": [
        " **One Hot Encoding for Columns which had lesser unique values and not ordinal**\n",
        "* One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.\n",
        "\n",
        "**Ordinal Encoding for Columns which has many unique categories** \n",
        "* Ordinal encoding is used here as label encoder is supported for column transformer.\n",
        "* Ordinal encoding is used for Ordinal Variable. Variable comprises a finite set of discrete values with a ranked ordering between values.\n",
        "\n",
        "**Standard Scaler** \n",
        "* Standardize features by removing the mean and scaling to unit variance.\n",
        "\n",
        "**Power Transformer**\n",
        "* Power transforms are a technique for transforming numerical input or output variables to have a Gaussian or more-Gaussian-like probability distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e245a3af",
      "metadata": {
        "id": "e245a3af"
      },
      "source": [
        "**Selecting number features for preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "ef40892b",
      "metadata": {
        "id": "ef40892b"
      },
      "outputs": [],
      "source": [
        "num_features = list(X.select_dtypes(exclude=\"object\").columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5321209d",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bccb0bc",
      "metadata": {
        "id": "2bccb0bc"
      },
      "source": [
        "### **Preprocessing using Column Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "e9a2c9b5",
      "metadata": {
        "id": "e9a2c9b5"
      },
      "outputs": [],
      "source": [
        "# Create Column Transformer with 3 types of transformers\n",
        "or_columns = ['has_job_experience','requires_job_training','full_time_position','education_of_employee']\n",
        "oh_columns = ['continent','unit_of_wage','region_of_employment']\n",
        "transform_columns= ['no_of_employees','company_age']\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler,OrdinalEncoder, PowerTransformer\n",
        "from sklearn.compose import ColumnTransformer \n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "numeric_transformer = StandardScaler()\n",
        "oh_transformer = OneHotEncoder()\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "\n",
        "transform_pipe = Pipeline(steps=[\n",
        "    ('transformer', PowerTransformer(method='yeo-johnson'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    [\n",
        "        (\"OneHotEncoder\", oh_transformer, oh_columns),\n",
        "        (\"Ordinal_Encoder\", ordinal_encoder, or_columns),\n",
        "        (\"Transformer\", transform_pipe, transform_columns),\n",
        "        (\"StandardScaler\", numeric_transformer, num_features)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "328d041f",
      "metadata": {
        "id": "328d041f"
      },
      "outputs": [],
      "source": [
        "X = preprocessor.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d791f12e",
      "metadata": {},
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b08df30",
      "metadata": {
        "id": "8b08df30"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "6688f6fb",
      "metadata": {
        "id": "6688f6fb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "# Resampling the minority class. The strategy can be changed as required.\n",
        "smt = SMOTEENN(random_state=42,sampling_strategy='minority' )\n",
        "# Fit the model to generate the data.\n",
        "X_res, y_res = smt.fit_resample(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a2c0485",
      "metadata": {
        "id": "8a2c0485"
      },
      "source": [
        "## Train Test Split\n",
        "- The train-test split procedure is used to estimate the performance of machine learning algorithms when they are used to make predictions on data not used to train the model.\n",
        "\n",
        "- It is a fast and easy procedure to perform, the results of which allow you to compare the performance of machine learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0f5858e",
      "metadata": {
        "id": "e0f5858e",
        "outputId": "d91dcb53-e0ae-4571-ef70-745f4c2ed1a2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import  train_test_split\n",
        "# separate dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res,y_res,test_size=0.2,random_state=42)\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "4cdb60f1",
      "metadata": {
        "id": "4cdb60f1"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report,ConfusionMatrixDisplay, \\\n",
        "                            precision_score, recall_score, f1_score, roc_auc_score,roc_curve \n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "b4da40e7",
      "metadata": {
        "id": "b4da40e7"
      },
      "outputs": [],
      "source": [
        "def evaluate_clf(true, predicted):\n",
        "    acc = accuracy_score(true, predicted) # Calculate Accuracy\n",
        "    f1 = f1_score(true, predicted) # Calculate F1-score\n",
        "    precision = precision_score(true, predicted) # Calculate Precision\n",
        "    recall = recall_score(true, predicted)  # Calculate Recall\n",
        "    roc_auc = roc_auc_score(true, predicted) #Calculate Roc\n",
        "    return acc, f1 , precision, recall, roc_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "91cebd1a",
      "metadata": {
        "id": "91cebd1a",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"K-Neighbors Classifier\": KNeighborsClassifier(),\n",
        "    \"XGBClassifier\": XGBClassifier(), \n",
        "    \"CatBoosting Classifier\": CatBoostClassifier(verbose=False),\n",
        "    \"Support Vector Classifier\": SVC(),\n",
        "    \"AdaBoost Classifier\": AdaBoostClassifier()\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "9aa69756",
      "metadata": {
        "id": "9aa69756"
      },
      "outputs": [],
      "source": [
        "# Create a function which can evaluate models and return a report \n",
        "def evaluate_models(X, y, models):\n",
        "    '''\n",
        "    This function takes in X and y and models dictionary as input\n",
        "    It splits the data into Train Test split\n",
        "    Iterates through the given model dictionary and evaluates the metrics\n",
        "    Returns: Dataframe which contains report of all models metrics with cost\n",
        "    '''\n",
        "    # separate dataset into train and test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "    \n",
        "    models_list = []\n",
        "    accuracy_list = []\n",
        "    auc= []\n",
        "    mlflow.set_experiment(\"baseline model 2\")\n",
        "    with mlflow.start_run(run_name=\"all_models_evaluation\") as parent_run:\n",
        "        print(\"Parent Run ID:\", parent_run.info.run_id)\n",
        "        for model_name, model in models.items():\n",
        "            with mlflow.start_run(run_name=model_name, nested=True):\n",
        "                model.fit(X_train, y_train)\n",
        "\n",
        "                # Make predictions\n",
        "                y_train_pred = model.predict(X_train)\n",
        "                y_test_pred = model.predict(X_test)\n",
        "\n",
        "                # Training set performance\n",
        "                model_train_accuracy, model_train_f1,model_train_precision,\\\n",
        "                model_train_recall,model_train_rocauc_score=evaluate_clf(y_train ,y_train_pred)\n",
        "                mlflow.log_metric('model_train_accuracy',model_train_accuracy)\n",
        "                mlflow.log_metric('model_train_f1', model_train_f1)\n",
        "                mlflow.log_metric('model_train_precision',model_train_precision)\n",
        "                mlflow.log_metric('model_train_recall',model_train_recall)\n",
        "                mlflow.log_metric('model_train_rocauc_score',model_train_rocauc_score)\n",
        "\n",
        "                # Test set performance\n",
        "                model_test_accuracy,model_test_f1,model_test_precision,\\\n",
        "                model_test_recall,model_test_rocauc_score=evaluate_clf(y_test, y_test_pred)\n",
        "                mlflow.log_metric('model_test_accuracy',model_test_accuracy)\n",
        "                mlflow.log_metric('model_test_f1', model_test_f1)\n",
        "                mlflow.log_metric('model_test_precision',model_test_precision)\n",
        "                mlflow.log_metric('model_test_recall',model_test_recall)\n",
        "                mlflow.log_metric('model_test_rocauc_score',model_test_rocauc_score)\n",
        "                # Collect results\n",
        "                models_list.append(model_name)\n",
        "                accuracy_list.append(model_test_accuracy)\n",
        "                auc.append(model_test_rocauc_score)\n",
        "                \n",
        "    report=pd.DataFrame(list(zip(models_list, accuracy_list)), columns=['Model Name', 'Accuracy']).sort_values(by=['Accuracy'], ascending=False)   \n",
        "    return report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e010280",
      "metadata": {
        "id": "0e010280"
      },
      "source": [
        "## Model Training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b08c217d",
      "metadata": {
        "id": "b08c217d",
        "outputId": "f8290c30-975a-4e44-80bd-9e120036e956",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "base_model_report =evaluate_models(X=X_res, y=y_res, models=models)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ac46818",
      "metadata": {
        "id": "8ac46818"
      },
      "source": [
        "**Results of All Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd686f5e",
      "metadata": {
        "id": "fd686f5e",
        "outputId": "bf5b1cc5-5eab-4a63-c765-0a23cdc94ab0"
      },
      "outputs": [],
      "source": [
        "base_model_report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3534db9",
      "metadata": {
        "id": "f3534db9"
      },
      "source": [
        "**Here we can use Random Forest for Hyper Parameter Tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b83fb1c",
      "metadata": {
        "id": "0b83fb1c"
      },
      "source": [
        "**Define the parameter distribution for Random forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "893c4b00",
      "metadata": {
        "id": "893c4b00"
      },
      "outputs": [],
      "source": [
        "#Initialize few parameter for Hyperparamter tuning\n",
        "xgboost_params = {\n",
        "    'max_depth':range(3,10,2),\n",
        "    'min_child_weight':range(1,6,2)\n",
        "}\n",
        "\n",
        "rf_params = {\n",
        "    \"max_depth\": [10, 12, None, 15, 20],\n",
        "    \"max_features\": ['sqrt', 'log2', None],\n",
        "    \"n_estimators\": [10, 50, 100, 200]\n",
        "}\n",
        "\n",
        "knn_params = {\n",
        "    \"algorithm\": ['auto', 'ball_tree', 'kd_tree','brute'],\n",
        "    \"weights\": ['uniform', 'distance'],\n",
        "    \"n_neighbors\": [3, 4, 5, 7, 9],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "9a57c5fd",
      "metadata": {
        "id": "9a57c5fd"
      },
      "outputs": [],
      "source": [
        "# Models list for Hyperparameter tuning\n",
        "randomcv_models = [\n",
        "    ('XGBoost', XGBClassifier(), xgboost_params),\n",
        "    (\"RF\", RandomForestClassifier(), rf_params),\n",
        "    (\"KNN\", KNeighborsClassifier(), knn_params)\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "723dea97",
      "metadata": {
        "id": "723dea97"
      },
      "source": [
        "**Create a function for model training and report which can be used in hyperparameter tuning loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5040517",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "mlflow.set_experiment('Hyperparameter Tuning - Notebook')\n",
        "model_param = {}\n",
        "\n",
        "with mlflow.start_run(run_name=\"All Models Tuning\") as parent_run:\n",
        "    for name, model_instance, param_grid in randomcv_models:\n",
        "        print(f\"\\nðŸ” Running GridSearchCV for: {name}\")\n",
        "\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=model_instance,\n",
        "            param_grid=param_grid,\n",
        "            cv=3,\n",
        "            verbose=2,\n",
        "            scoring='f1',\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        grid_search.fit(X_res, y_res)\n",
        "\n",
        "        # Log each combination as a nested run\n",
        "        for param_set, mean_score, std_score in zip(\n",
        "            grid_search.cv_results_['params'],\n",
        "            grid_search.cv_results_['mean_test_score'],\n",
        "            grid_search.cv_results_['std_test_score']\n",
        "        ):\n",
        "            with mlflow.start_run(run_name=f\"{name} | {param_set}\", nested=True):\n",
        "                model = model_instance.__class__(**param_set)\n",
        "                model.fit(X_train, y_train)\n",
        "                y_train_pred = model.predict(X_train)\n",
        "                y_test_pred = model.predict(X_test)\n",
        "\n",
        "                # Train set performance\n",
        "                model_train_accuracy, model_train_f1, model_train_precision, \\\n",
        "                model_train_recall, model_train_rocauc_score = evaluate_clf(y_train, y_train_pred)\n",
        "                mlflow.log_metric('model_train_accuracy', model_train_accuracy)\n",
        "                mlflow.log_metric('model_train_f1', model_train_f1)\n",
        "                mlflow.log_metric('model_train_precision', model_train_precision)\n",
        "                mlflow.log_metric('model_train_recall', model_train_recall)\n",
        "                mlflow.log_metric('model_train_rocauc_score', model_train_rocauc_score)\n",
        "\n",
        "                # Test set performance\n",
        "                model_test_accuracy, model_test_f1, model_test_precision, \\\n",
        "                model_test_recall, model_test_rocauc_score = evaluate_clf(y_test, y_test_pred)\n",
        "                mlflow.log_metric('model_test_accuracy', model_test_accuracy)\n",
        "                mlflow.log_metric('model_test_f1', model_test_f1)\n",
        "                mlflow.log_metric('model_test_precision', model_test_precision)\n",
        "                mlflow.log_metric('model_test_recall', model_test_recall)\n",
        "                mlflow.log_metric('model_test_rocauc_score', model_test_rocauc_score)\n",
        "\n",
        "        # Log best model and parameters\n",
        "        best_model = grid_search.best_estimator_\n",
        "        best_params = grid_search.best_params_\n",
        "        best_f1 = grid_search.best_score_\n",
        "        model_param[name] = best_params\n",
        "\n",
        "        mlflow.log_params({f\"{name}_best_\" + k: v for k, v in best_params.items()})\n",
        "        mlflow.log_metric(f\"{name}_best_f1\", best_f1)\n",
        "        #mlflow.sklearn.log_model(best_model, artifact_path=f\"{name}_best_model\")\n",
        "\n",
        "        print(f\"ðŸ† Best {name}: Params = {best_params}, F1 = {best_f1:.4f}\")\n",
        "\n",
        "# Print best parameters\n",
        "for model_name in model_param:\n",
        "    print(f\"\\n---------------- Best Params for {model_name} -------------------\")\n",
        "    print(model_param[model_name])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83b5de27",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_param"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "616ebc0a",
      "metadata": {
        "id": "616ebc0a"
      },
      "source": [
        "## Retraining the Model with best Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "a4a781fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_models2(X, y, models, model_param):\n",
        "    '''\n",
        "    This function evaluates multiple models with given hyperparameters.\n",
        "    \n",
        "    Args:\n",
        "        X (pd.DataFrame): Feature data\n",
        "        y (pd.Series): Target labels\n",
        "        models (dict): Dictionary of model_name: model_object\n",
        "        model_param (dict): Dictionary of model_name: hyperparameter dict\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Report of accuracy and AUC for all models\n",
        "    '''\n",
        "  \n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    \n",
        "    models_list = []\n",
        "    accuracy_list = []\n",
        "    auc = []\n",
        "\n",
        "    mlflow.set_experiment(\"Hyper parameter tuning\")\n",
        "\n",
        "    with mlflow.start_run(run_name=\"all_models_evaluation\") as parent_run:\n",
        "        print(\"Parent Run ID:\", parent_run.info.run_id)\n",
        "\n",
        "        for model_name in models:\n",
        "            model = models[model_name]\n",
        "            param = model_param.get(model_name, {})\n",
        "            loggable_params = {\n",
        "                f\"{model_name}_{k}\": \"None\" if v is None else v\n",
        "                for k, v in param.items()\n",
        "            }\n",
        "\n",
        "            with mlflow.start_run(run_name=model_name, nested=True):\n",
        "\n",
        "\n",
        "                # Log hyperparameters\n",
        "                mlflow.log_params(loggable_params)\n",
        "\n",
        "                # Fit model\n",
        "                model.fit(X_train, y_train)\n",
        "\n",
        "                # Predictions\n",
        "                y_train_pred = model.predict(X_train)\n",
        "                y_test_pred = model.predict(X_test)\n",
        "\n",
        "                # Train performance\n",
        "                model_train_accuracy, model_train_f1, model_train_precision, \\\n",
        "                model_train_recall, model_train_rocauc_score = evaluate_clf(y_train, y_train_pred)\n",
        "\n",
        "                mlflow.log_metric('model_train_accuracy', model_train_accuracy)\n",
        "                mlflow.log_metric('model_train_f1', model_train_f1)\n",
        "                mlflow.log_metric('model_train_precision', model_train_precision)\n",
        "                mlflow.log_metric('model_train_recall', model_train_recall)\n",
        "                mlflow.log_metric('model_train_rocauc_score', model_train_rocauc_score)\n",
        "\n",
        "                # Test performance\n",
        "                model_test_accuracy, model_test_f1, model_test_precision, \\\n",
        "                model_test_recall, model_test_rocauc_score = evaluate_clf(y_test, y_test_pred)\n",
        "\n",
        "                mlflow.log_metric('model_test_accuracy', model_test_accuracy)\n",
        "                mlflow.log_metric('model_test_f1', model_test_f1)\n",
        "                mlflow.log_metric('model_test_precision', model_test_precision)\n",
        "                mlflow.log_metric('model_test_recall', model_test_recall)\n",
        "                mlflow.log_metric('model_test_rocauc_score', model_test_rocauc_score)\n",
        "                \n",
        "                # Store results\n",
        "                models_list.append(model_name)\n",
        "                accuracy_list.append(model_test_accuracy)\n",
        "                auc.append(model_test_rocauc_score)\n",
        "\n",
        "    report = pd.DataFrame(\n",
        "        list(zip(models_list, accuracy_list, auc)),\n",
        "        columns=['Model Name', 'Accuracy', 'AUC']\n",
        "    ).sort_values(by='Accuracy', ascending=False)\n",
        "\n",
        "    return report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5738ae2c",
      "metadata": {
        "id": "5738ae2c",
        "outputId": "25600fb4-c963-4bf7-ba2c-d02cf4d5962e"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score,roc_curve\n",
        "best_models = {\n",
        "    \"RF\": RandomForestClassifier(**model_param['RF']),\n",
        "    \"KNN\": KNeighborsClassifier(**model_param['KNN']),\n",
        "    \"XGBoost\": XGBClassifier(**model_param['XGBoost'], n_jobs=-1),\n",
        "}\n",
        "tuned_report =evaluate_models2(X=X_res, y=y_res, models=best_models,model_param=model_param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ebcc3e2",
      "metadata": {
        "id": "3ebcc3e2",
        "outputId": "870a295b-85c5-4204-cb8f-178bc6ccd9a6"
      },
      "outputs": [],
      "source": [
        "tuned_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4beb9bb",
      "metadata": {
        "id": "a4beb9bb",
        "outputId": "fde92afe-af72-428a-e4d9-d44a094b28ad"
      },
      "outputs": [],
      "source": [
        "best_model = KNeighborsClassifier(**model_param['KNN'])\n",
        "best_model = best_model.fit(X_train,y_train)\n",
        "y_pred = best_model.predict(X_test)\n",
        "score = accuracy_score(y_test,y_pred)\n",
        "cr = classification_report(y_test,y_pred)\n",
        "\n",
        "print(\"FINAL MODEL 'KNN'\")\n",
        "print (\"Accuracy Score value: {:.4f}\".format(score))\n",
        "print (cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bd89efe",
      "metadata": {
        "id": "8bd89efe",
        "outputId": "3078255a-0670-492a-8d4a-ab6aa891ef2f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84d34671",
      "metadata": {
        "id": "84d34671"
      },
      "source": [
        "## Best Model is K-Nearest Neighbor(KNN) with Accuracy 96.83%"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0a82949",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "e6422fc631fadc7a14dbe43d10d544a88c5583408f68c588bbe9704b9c540dab"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
